07:51:52    tf version 1.0.1
07:52:40        t=0: train policy score: 8.27640300989
07:52:47    t=0: Eval policy score: 4.38767024875
07:55:15        t=10: train policy score: 8.82864525914
07:57:41        t=20: train policy score: 5.81194257736
08:00:06        t=30: train policy score: 9.39142385125
08:02:35        t=40: train policy score: 9.29887604713
08:05:02        t=50: train policy score: 9.28883558512
08:07:29        t=60: train policy score: 9.24077787995
08:09:59        t=70: train policy score: 9.125426054
08:12:27        t=80: train policy score: 9.18184235692
08:14:54        t=90: train policy score: 9.37453299761
08:17:22        t=100: train policy score: 9.41556051373
08:17:29    t=100: Eval policy score: 7.89142936468
08:19:56        t=110: train policy score: 9.40970686078
08:22:25        t=120: train policy score: 9.31433090568
08:24:55        t=130: train policy score: 9.24749222398
08:27:21        t=140: train policy score: 9.32395318151
08:29:49        t=150: train policy score: 9.2110221684
08:32:18        t=160: train policy score: 9.2474822998
08:34:47        t=170: train policy score: 9.37053859234
08:37:14        t=180: train policy score: 9.19032049179
08:39:41        t=190: train policy score: 9.19020965695
08:42:09        t=200: train policy score: 9.29931291938
08:42:16    t=200: Eval policy score: 7.8815715611
08:44:45        t=210: train policy score: 9.26803389192
02:23:50    tf version 1.0.1
02:23:59    Traceback (most recent call last):
  File "run_full_model.py", line 158, in <module>
    main()
  File "run_full_model.py", line 107, in main
    with tf.Session() as session:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1176, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 552, in __init__
    self._session = tf_session.TF_NewDeprecatedSession(opts, status)
KeyboardInterrupt

02:24:14    tf version 1.0.1
02:26:20    Traceback (most recent call last):
  File "run_full_model.py", line 158, in <module>
    main()
  File "run_full_model.py", line 124, in main
    learning, res = run_model(model, session, c_model.get_next_info, test=True, collect_extra_data=True)
  File "run_full_model.py", line 38, in run_model
    all_Qs_stats = np.zeros((batch_size, seq_lens, num_topics))
TypeError: only integer scalar arrays can be converted to a scalar index

02:30:17    tf version 1.0.1
02:30:28    Traceback (most recent call last):
  File "run_full_model.py", line 158, in <module>
    main()
  File "run_full_model.py", line 124, in main
    learning, res = run_model(model, session, c_model.get_next_info, test=True, collect_extra_data=True)
  File "run_full_model.py", line 38, in run_model
    all_Qs_stats = np.zeros((batch_size, seq_lens, num_topics))
TypeError: only integer scalar arrays can be converted to a scalar index

02:31:55    tf version 1.0.1
02:32:07    Traceback (most recent call last):
  File "run_full_model.py", line 158, in <module>
    main()
  File "run_full_model.py", line 124, in main
    learning, res = run_model(model, session, c_model.get_next_info, test=True, collect_extra_data=True)
  File "run_full_model.py", line 60, in run_model
    delta_learning[:, j] = total_learning - total_learning[:, j-1]   # just a bit lazy
ValueError: operands could not be broadcast together with shapes (128,100) (128,) 

02:32:51    tf version 1.0.1
02:33:14    random policy score 0 0: 4.08107176423
02:33:14    Traceback (most recent call last):
  File "run_full_model.py", line 158, in <module>
    main()
  File "run_full_model.py", line 126, in main
    alexs_data[0, model_no, BATCH_SIZE * batch_num: (batch_num + 1) * BATCH_SIZE] = res
ValueError: could not broadcast input array from shape (128) into shape (128,124)

02:34:49    tf version 1.0.1
02:34:49    Traceback (most recent call last):
  File "run_full_model.py", line 158, in <module>
    main()
  File "run_full_model.py", line 100, in main
    topics, answers, num_topics = dkt_tf.read_assistments_data(dkt_tf.DATA_LOC)
  File "/home/cs234/cs234_project/src/dkt_tf.py", line 22, in read_assistments_data
    data = [[int(y) for y in x.strip().split()] for x in f.readlines()]
KeyboardInterrupt

02:35:04    tf version 1.0.1
02:35:26    random policy score 0 0: 3.74960160255
02:35:37    random policy score 0 1: 3.69100299478
02:35:47    random policy score 1 0: 6.90672150254
02:35:58    random policy score 1 1: 7.05861288309
02:35:58    Traceback (most recent call last):
  File "run_full_model.py", line 158, in <module>
    main()
  File "run_full_model.py", line 130, in main
    learning = run_model(model, session, train_critic.get_next_info)
  File "run_full_model.py", line 66, in run_model
    if not test: model.apply_grad(session, cur_learning)
  File "/home/cs234/cs234_project/src/actor.py", line 143, in apply_grad
    action_vec[i, self.next_action[i]] = val
ValueError: setting an array element with a sequence.

02:37:19    tf version 1.0.1
02:37:41    random policy score 0 0: 3.63690572977
02:37:52    random policy score 0 1: 4.44601756334
02:38:03    random policy score 1 0: 6.86014863849
02:38:13    random policy score 1 1: 6.71450197697
02:38:31        t=0: train policy score: 3.74815678596
02:38:42    t=0: Eval policy score: 6.65053021908
02:41:39        t=10: train policy score: 4.36973872781
02:44:34        t=20: train policy score: 3.55621114373
02:47:30        t=30: train policy score: 7.5584769249
02:50:25        t=40: train policy score: 3.085493505
02:53:20        t=50: train policy score: 10.646987021
02:56:16        t=60: train policy score: 10.3518284857
02:59:11        t=70: train policy score: 7.96980959177
03:02:09        t=80: train policy score: 1.46272355318
03:05:06        t=90: train policy score: 10.4017452598
03:08:05        t=100: train policy score: -4.05085077882
03:08:15    t=100: Eval policy score: -2.72987037897
03:11:11        t=110: train policy score: 9.11007273197
03:14:08        t=120: train policy score: 10.3127334416
03:17:04        t=130: train policy score: 10.3352165818
03:20:00        t=140: train policy score: 10.5086063147
03:23:00        t=150: train policy score: 10.3716691732
03:25:57        t=160: train policy score: 10.345600158
03:28:53        t=170: train policy score: 10.3995329738
03:31:48        t=180: train policy score: 10.5699451864
03:34:45        t=190: train policy score: 10.5642748773
03:37:44        t=200: train policy score: 10.4612818062
03:37:54    t=200: Eval policy score: -5.83463522792
03:40:49        t=210: train policy score: 10.6100000739
03:43:44        t=220: train policy score: 10.5231072903
03:46:40        t=230: train policy score: 10.5519036949
03:49:34        t=240: train policy score: 10.5753676891
03:52:31        t=250: train policy score: 10.5168564022
03:55:29        t=260: train policy score: 10.6539124846
03:58:27        t=270: train policy score: 10.4808152914
04:01:24        t=280: train policy score: 10.462551415
04:04:22        t=290: train policy score: 10.3582960665
04:07:19        t=300: train policy score: 10.3521231115
04:07:29    t=300: Eval policy score: -5.72163024545
04:10:26        t=310: train policy score: 10.4907336533
04:13:22        t=320: train policy score: 10.4521527886
04:16:19        t=330: train policy score: 10.4736227095
04:19:17        t=340: train policy score: 10.2698312402
04:22:17        t=350: train policy score: 10.3890534639
04:25:14        t=360: train policy score: 10.2859609723
04:28:12        t=370: train policy score: 10.245556891
04:31:08        t=380: train policy score: 10.452316463
04:34:08        t=390: train policy score: 10.3335605562
04:37:04        t=400: train policy score: 10.5256497264
04:37:14    t=400: Eval policy score: -5.67090445757
04:40:11        t=410: train policy score: 10.4273903668
04:43:08        t=420: train policy score: 10.4412126839
04:46:05        t=430: train policy score: 10.4951369762
04:49:02        t=440: train policy score: 10.1716877222
04:51:59        t=450: train policy score: 10.3160036802
04:54:55        t=460: train policy score: 10.4891444445
04:57:51        t=470: train policy score: 10.5663625896
05:00:47        t=480: train policy score: 10.5187117159
05:03:44        t=490: train policy score: 10.2999815345
05:08:22    tf version 1.0.1
05:08:45    random policy score 0 0: 3.74020639062
05:08:55    random policy score 0 1: 3.9604947865
05:09:06    random policy score 1 0: 6.48583978415
05:09:16    random policy score 1 1: 6.33954718709
05:09:34        t=0: train policy score: 4.21365398169
05:09:45    t=0: Eval policy score: 6.8099539578
05:12:44        t=10: train policy score: 3.81227767467
05:15:43        t=20: train policy score: 4.00385174155
05:15:53    t=20: Eval policy score: 7.25300979614
05:18:51        t=30: train policy score: 4.0466593802
05:21:50        t=40: train policy score: 4.40754178166
05:22:01    t=40: Eval policy score: 5.64977124333
05:25:00        t=50: train policy score: 2.93334209919
05:27:58        t=60: train policy score: 4.24961662292
05:28:09    t=60: Eval policy score: 8.42278957367
05:31:09        t=70: train policy score: 2.44846078753
05:34:07        t=80: train policy score: 2.98989376426
05:34:17    t=80: Eval policy score: 7.79132911563
05:37:15        t=90: train policy score: -0.451615095139
05:40:14        t=100: train policy score: -3.03578066826
05:40:24    t=100: Eval policy score: 5.95296499133
05:43:22        t=110: train policy score: -1.71337521076
05:46:22        t=120: train policy score: -1.8305195868
05:46:32    t=120: Eval policy score: 5.97949299216
05:49:31        t=130: train policy score: -1.72413378954
05:52:28        t=140: train policy score: -0.446557462215
05:52:39    t=140: Eval policy score: 1.92328444123
05:55:36        t=150: train policy score: 5.55003082752
05:58:35        t=160: train policy score: 1.27108702064
05:58:45    t=160: Eval policy score: 9.66550981998
06:01:44        t=170: train policy score: 3.85171598196
06:04:42        t=180: train policy score: 0.509270727634
06:04:52    t=180: Eval policy score: -4.05286452174
06:07:50        t=190: train policy score: -4.20583868027
06:10:41    Final eval network policy score -2.53960245848
06:10:51    actor policy score 0 0: -4.30199447274
06:11:02    actor policy score 0 1: -4.25123047829
06:11:12    actor policy score 1 0: -2.59873080254
06:11:23    actor policy score 1 1: -2.53566858172
07:43:19    tf version 1.0.1
07:43:40    random policy score 0 0: 3.66824540496
07:43:50    random policy score 0 1: 3.9017059207
07:44:01    random policy score 1 0: 6.6556108892
07:44:11    random policy score 1 1: 6.63124218583
07:44:28        t=0: train policy score: 4.38541117311
07:44:38    t=0: Eval policy score: 6.70835000277
07:47:34        t=10: train policy score: 3.44852539897
07:50:29        t=20: train policy score: 3.50905632973
07:50:39    t=20: Eval policy score: 6.88546836376
07:53:33        t=30: train policy score: 4.7574249804
07:56:29        t=40: train policy score: 0.587648123503
07:56:39    t=40: Eval policy score: 4.80742898583
07:59:34        t=50: train policy score: -3.02447408438
08:01:24    Traceback (most recent call last):
  File "run_full_model.py", line 158, in <module>
    main()
  File "run_full_model.py", line 130, in main
    learning = run_model(model, session, train_critic.get_next_info)
  File "run_full_model.py", line 55, in run_model
    cur_learning, action_correct = critic_fn(session, seq_lens, correct_hist, q_hist)
  File "run_full_model.py", line 83, in get_next_info
    new_probs = self.critic.next_probs(session, lens, mask, prev_answers, prev_topics)
  File "/home/cs234/cs234_project/src/dkt_tf.py", line 220, in next_probs
    post_probs, = session.run([self.post_probs], feed_dict=feed_dict)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 767, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 965, in _run
    feed_dict_string, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1015, in _do_run
    target_list, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1022, in _do_call
    return fn(*args)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1004, in _run_fn
    status, run_metadata)
KeyboardInterrupt

08:06:14    tf version 1.0.1
08:06:36    random policy score 0 0: 4.06378802657
08:06:46    random policy score 0 1: 4.42587545514
08:06:57    random policy score 1 0: 6.2212356925
08:07:07    random policy score 1 1: 6.39777383208
08:07:26        t=0: train policy score: 4.04441136122
08:07:36    t=0: Eval policy score: 6.30163106322
08:10:36        t=10: train policy score: 4.70570471883
08:13:35        t=20: train policy score: 4.68908879161
08:13:45    t=20: Eval policy score: 10.2550626397
08:16:41        t=30: train policy score: 4.4972153604
08:19:37        t=40: train policy score: 4.71660169959
08:19:48    t=40: Eval policy score: 10.387332201
08:22:45        t=50: train policy score: 4.6300830543
08:25:44        t=60: train policy score: 4.67176428437
08:25:54    t=60: Eval policy score: 10.2719438672
08:28:52        t=70: train policy score: 4.67066600919
08:31:48        t=80: train policy score: 4.73053461313
08:31:58    t=80: Eval policy score: 10.3089128435
08:34:55        t=90: train policy score: 4.54005840421
08:37:54        t=100: train policy score: 4.49078428745
08:38:05    t=100: Eval policy score: 10.1973228753
08:41:03        t=110: train policy score: 4.627856493
08:43:59        t=120: train policy score: 4.55836856365
08:44:09    t=120: Eval policy score: 10.0492545664
08:47:08        t=130: train policy score: 4.72092807293
08:50:04        t=140: train policy score: 4.66348141432
08:50:14    t=140: Eval policy score: 10.3060573339
08:53:14        t=150: train policy score: 4.53660878539
08:56:12        t=160: train policy score: 4.47494006157
08:56:22    t=160: Eval policy score: 10.2712859511
08:59:19        t=170: train policy score: 4.7049433589
09:02:16        t=180: train policy score: 4.69540989399
09:02:26    t=180: Eval policy score: 10.1469523907
09:05:22        t=190: train policy score: 4.61226412654
09:08:12    Final eval network policy score 10.3216767609
09:08:22    actor policy score 0 0: 4.60098689795
09:08:32    actor policy score 0 1: 4.6505972445
09:08:43    actor policy score 1 0: 10.3126117587
09:08:53    actor policy score 1 1: 10.1888785362
09:11:26    tf version 1.0.1
09:13:41    random policy score 0 0: 7.06050527096
09:13:51    random policy score 0 1: 6.71291783452
09:14:01    random policy score 1 0: 3.4707928896
09:14:11    random policy score 1 1: 4.32079312205
09:14:29        t=0: train policy score: 7.020319134
09:14:39    t=0: Eval policy score: 4.15171465278
09:17:33        t=10: train policy score: 12.3435199857
09:20:29        t=20: train policy score: 12.3359267414
09:20:39    t=20: Eval policy score: 0.715367585421
09:23:36        t=30: train policy score: 12.4040149748
09:26:32        t=40: train policy score: 11.9245592952
09:26:42    t=40: Eval policy score: 4.15756648779
09:29:37        t=50: train policy score: 12.3874828815
09:32:33        t=60: train policy score: 12.2854299545
09:32:43    t=60: Eval policy score: 2.0585770905
09:35:37        t=70: train policy score: 2.03347188234
09:38:32        t=80: train policy score: 2.17679381371
09:38:42    t=80: Eval policy score: 7.65860018134
09:41:37        t=90: train policy score: 1.91473647952
09:44:34        t=100: train policy score: 1.86009573936
09:44:44    t=100: Eval policy score: 7.62220820785
09:47:40        t=110: train policy score: 1.79207843542
09:50:35        t=120: train policy score: 1.84212318063
09:50:45    t=120: Eval policy score: 7.76758864522
09:53:41        t=130: train policy score: 2.16168937087
09:56:35        t=140: train policy score: 2.05377471447
09:56:45    t=140: Eval policy score: 7.9045369029
09:59:41        t=150: train policy score: 2.02390599251
10:02:38        t=160: train policy score: 2.00250458717
10:02:49    t=160: Eval policy score: 7.79876247048
10:05:45        t=170: train policy score: 4.3577773869
10:08:40        t=180: train policy score: 1.93899738789
10:08:50    t=180: Eval policy score: 7.68229231238
10:11:45        t=190: train policy score: 2.04337736964
10:14:32    Final eval network policy score 7.7394464016
10:14:42    actor policy score 0 0: 1.96876344085
10:14:53    actor policy score 0 1: 2.11418414116
10:15:03    actor policy score 1 0: 7.66905331612
10:15:13    actor policy score 1 1: 7.72701245546
